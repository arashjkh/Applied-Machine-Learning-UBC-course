{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Applied Machine Learning \n",
    "## Project â€“ Stage 2 (Sorting Legos Using Engineered Features)\n",
    "\n",
    "You have been hired by a company to develop machine learning algorithms for a sorting facility. The requirement is that the sorting device takes images of items on a conveyor belt, and then uses and machine learning algorithm to classify the items into classes. Then, the items get routed through different routes on the conveyor belt depending on their class.\n",
    "\n",
    "For the sake of this project, you are given RGB images, and your focus will be on developing the classification algorithms. Also, for simplicity, it is assumed that items are Lego pieces of three different types with the following shapes (top view): Rectangles (2x4), squares (2x2), and circles (2x2). Examples are shown below:\n",
    "\n",
    "<img align=\"center\" src=\"Lego.JPG\">\n",
    "\n",
    "The company tested your solution from stage 1, and requested an update. Namely, the company realized they have given you an idealized data set where the images are centered and oriented in a given direction. However, the items may appear in the image off-centre, and may have various orientations as shown below:\n",
    "\n",
    "<img align=\"center\" src=\"Lego2.JPG\">\n",
    "\n",
    "Your algorithm should be able to classify these three classes with different centering and orientations. To realize this, you need to extract/engineer features from the images, and to use these features to train and test a machine learning algorithm. The company requested that you use these features as the input of a 3-class classifier (Logistic regression, as in Lecture 5), and to use a maximum of 64 features per image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The required libraries are imported here\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell contains the functions defined to read the images from the given directories,\n",
    "# separate them into classes, and preprocess them, so they can be used by thr model.\n",
    "\n",
    "def image_cropper(img: np.array, height_percentage: int = 40, width_percentage: int = 40) -> np.array:\n",
    "    \"\"\"\n",
    "    the image is cropped from both sides of the height and width\n",
    "    :param img: the images as an np.array\n",
    "    :param width_percentage: the percentage of the pixels from the width of the image we would like to crop\n",
    "    :param height_percentage: the percentage of the pixels from the height of the image we would like to crop\n",
    "    :return: cropped image as np.array\n",
    "    \"\"\"\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    cropped_height_amount = height // 100 * height_percentage\n",
    "    cropped_width_amount = height // 100 * width_percentage\n",
    "    img = img[cropped_height_amount // 2:height - cropped_height_amount // 2,\n",
    "          cropped_width_amount // 2:width - cropped_width_amount // 2]\n",
    "    return img\n",
    "\n",
    "\n",
    "def dynamic_image_cropper(img: np.array, region_of_interest=500, negative_image = False) -> np.array:\n",
    "    \"\"\"\n",
    "    The pixel value mean is calculated at the 4 corners of the image + the center.\n",
    "    The region with the lowest mean (since pixels of the object are darker) is chosen as the region of interest.\n",
    "    In case of negative image, we use the maximum.\n",
    "    :param img: img as np.array\n",
    "    :param region_of_interest: the size of the bounding box we are checking for calculating the mean\n",
    "    :return: cropped image\n",
    "    \"\"\"\n",
    "    if negative_image:\n",
    "        img = abs(255-img)\n",
    "    # points of interest\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    center = [height // 2, width // 2]\n",
    "    right_top_corner = [0, width - 1]\n",
    "    left_top_corner = [0, 0]\n",
    "    right_bottom_corner = [height - 1, width - 1]\n",
    "    left_bottom_corner = [height - 1, 0]\n",
    "\n",
    "    # calculating the mean\n",
    "    left_bottom_corner_mean = np.mean(img[left_bottom_corner[0] - region_of_interest:left_bottom_corner[0],\n",
    "                                      left_bottom_corner[1]:left_bottom_corner[1] + region_of_interest])\n",
    "    left_top_corner_mean = np.mean(img[left_top_corner[0]: region_of_interest + left_bottom_corner[0],\n",
    "                                   left_bottom_corner[1]:left_bottom_corner[1] + region_of_interest])\n",
    "    right_top_corner_mean = np.mean(img[right_top_corner[0]:right_top_corner[0] + region_of_interest,\n",
    "                                    right_top_corner[1] - region_of_interest: right_top_corner[1]])\n",
    "    right_bottom_corner_mean = np.mean(img[right_bottom_corner[0] - region_of_interest:right_bottom_corner[0],\n",
    "                                       right_bottom_corner[1] - region_of_interest:right_bottom_corner[1]])\n",
    "    center_mean = np.mean(img[center[0] - region_of_interest // 2:center[0] + region_of_interest // 2,\n",
    "                          center[1] - region_of_interest // 2:center[1] + region_of_interest // 2])\n",
    "\n",
    "\n",
    "    # getting the minimum/maximum mean and associated region\n",
    "    if not negative_image:\n",
    "        min_mean = np.argmin(np.array(\n",
    "            [center_mean, left_top_corner_mean, left_bottom_corner_mean, right_top_corner_mean, right_bottom_corner_mean]))\n",
    "    else:\n",
    "        min_mean = np.argmax(np.array(\n",
    "            [center_mean, left_top_corner_mean, left_bottom_corner_mean, right_top_corner_mean, right_bottom_corner_mean]))\n",
    "    if min_mean == 0:\n",
    "        return img[center[0] - region_of_interest // 2:center[0] + region_of_interest // 2,\n",
    "               center[1] - region_of_interest // 2:center[1] + region_of_interest // 2]\n",
    "    elif min_mean == 1:\n",
    "        return img[left_top_corner[0]: region_of_interest + left_bottom_corner[0],\n",
    "               left_bottom_corner[1]:left_bottom_corner[1] + region_of_interest]\n",
    "    elif min_mean == 2:\n",
    "        return img[left_bottom_corner[0] - region_of_interest:left_bottom_corner[0],\n",
    "               left_bottom_corner[1]:left_bottom_corner[1] + region_of_interest]\n",
    "    elif min_mean == 3:\n",
    "        return img[right_top_corner[0]:right_top_corner[0] + region_of_interest,\n",
    "               right_top_corner[1] - region_of_interest: right_top_corner[1]]\n",
    "    elif min_mean == 4:\n",
    "        return img[right_bottom_corner[0] - region_of_interest:right_bottom_corner[0],\n",
    "               right_bottom_corner[1] - region_of_interest:right_bottom_corner[1]]\n",
    "\n",
    "def dynamic_image_cropper2(img):\n",
    "    \"\"\"\n",
    "    Find the object in the image and isolates it based on the edges and contours in the image.\n",
    "    The image is first turned into its negative counter part.\n",
    "    The the edges on the negative image are calculated, the contours for the edge image are found,\n",
    "    and the contour image with the biggest area is chosen as the final image and the object.\n",
    "    :param img:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    img = np.uint8(img)\n",
    "    gray_negative = abs(255-img)\n",
    "    img = gray_negative\n",
    "    # tested blured version of image for source, noise reduction and unsharp mask\n",
    "    blur = cv2.GaussianBlur(img, (5,5), cv2.BORDER_DEFAULT)\n",
    "    # img = img + (img-blur)\n",
    "    edges= cv2.Canny(img, 100,200)\n",
    "    contours, hierarchy= cv2.findContours(edges.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    sorted_contours= sorted(contours, key=cv2.contourArea, reverse= True)\n",
    "\n",
    "    objects = []\n",
    "    for (i,c) in enumerate(sorted_contours):\n",
    "        x,y,w,h= cv2.boundingRect(c)\n",
    "        new = img[y- 10:y+h + 10, x- 10:x+w + 10]\n",
    "        new[new<110] = 0\n",
    "        objects.append(new)\n",
    "    biggest_object = np.array([[1,1],[1,1]])\n",
    "    for obj in objects:\n",
    "        if obj.shape[0] * obj.shape[1] > biggest_object.shape[0] * biggest_object.shape[1]:\n",
    "            biggest_object = obj\n",
    "    # cv2.imshow(\"test\", biggest_object)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "    return biggest_object\n",
    "\n",
    "\n",
    "\n",
    "def image_scaler(img, new_height=20, new_width=20):\n",
    "    \"\"\"\n",
    "    :param img: the images as an np.array\n",
    "    :param new_height: the new height of the scaled image\n",
    "    :param new_width: the new width of the scaled image\n",
    "    :return: scaled image as np.array\n",
    "    \"\"\"\n",
    "    img = Image.fromarray(img)\n",
    "    img = img.resize((new_width, new_height))\n",
    "    img = np.array(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def image_standardization(img, max_value=255):\n",
    "    \"\"\"\n",
    "    changes the values of the image pixels to be between 0 and max_value. The commented line is for discretizing the pixel values\n",
    "    :param max_value: max_value of the new pixels\n",
    "    :param img: the images as an np.array\n",
    "    :return: standardized image\n",
    "    \"\"\"\n",
    "    img = (img - np.min(img)) / (np.max(img) - np.min(img)) * max_value\n",
    "    # img = (img // 16) * 16\n",
    "    return img\n",
    "\n",
    "\n",
    "def separate_classes(img_files, name_separation_chars=3, test_flag=False, classes_dict=None):\n",
    "    \"\"\"\n",
    "    separates the images into classes based on their file names\n",
    "    :param img_files: list of image file names\n",
    "    :param name_separation_chars: number of characters in the name that should get used for separation\n",
    "    :return: a list of classes for image names, a dict containing the name and the corresponding class number of images\n",
    "    \"\"\"\n",
    "    names = list(set(img_name[:name_separation_chars] for img_name in img_files))\n",
    "    if not test_flag:\n",
    "        classes_dict = dict(enumerate(names))\n",
    "        classes_dict = {v: k for k, v in classes_dict.items()}\n",
    "    classes = []\n",
    "    for img_name in img_files:\n",
    "        classes.append(classes_dict[img_name[:name_separation_chars]])\n",
    "    return classes, classes_dict\n",
    "\n",
    "\n",
    "def get_files_list(data_dir, name_separation_chars=3, test_flag=False, classes_dict=None):\n",
    "    \"\"\"\n",
    "    gets the list of image names from the directory\n",
    "    :param data_dir: path to the image files\n",
    "    :return: list of image names, their classes and the dict for classes\n",
    "    \"\"\"\n",
    "    data = os.listdir(data_dir)\n",
    "    classes, classes_dict = separate_classes(data, name_separation_chars=name_separation_chars, test_flag=test_flag,\n",
    "                                             classes_dict=classes_dict)\n",
    "    return data, classes, classes_dict\n",
    "\n",
    "\n",
    "def image_reader(img_dir):\n",
    "    \"\"\"\n",
    "    reads image in grayscale\n",
    "    :return: img as np.array\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_dir)\n",
    "    gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    return gray\n",
    "\n",
    "\n",
    "def calculate_image_edges(img, total_extracted_features, threshold=100, peak_threshold=0.3, cutoff_percentage=20):\n",
    "    \"\"\"\n",
    "    Was supposed to be similar to the landing practice coding example in the course.\n",
    "    PIL filters are used to find the edges from the image.\n",
    "    Then the sum of edges are calculated for vertical and horizontal axis.\n",
    "    Did not return good results for this project.\n",
    "    Was changed to combining the max values in the vertical and horizontal edges\n",
    "    :param img: img as PIL object\n",
    "    :param total_extracted_features: number of features we want to extract from the image\n",
    "    :param threshold: used for noise reduction\n",
    "    :param peak_threshold: used for high-pass filter\n",
    "    :param cutoff_percentage: used for image resizing and reduction of zero pixels after edge detection\n",
    "    :return: engineered features from the image\n",
    "    \"\"\"\n",
    "    # img = Image.fromarray(img)\n",
    "    img_edges = img.filter(ImageFilter.FIND_EDGES)\n",
    "    edge_array = np.array(img_edges)\n",
    "    edge_array = edge_array[4:edge_array.shape[0] - 4, 4:edge_array.shape[1] - 4]\n",
    "    edge_array[edge_array < threshold] = 0\n",
    "    edges_v = np.sum(edge_array, axis=0).astype(float)\n",
    "    edges_h = np.sum(edge_array, axis=1).astype(float)\n",
    "    ##############################################################################################################\n",
    "    ## High-pass filter\n",
    "    # M = 4\n",
    "    # edges_v_copy = edges_v.copy()\n",
    "    # for i in range(M // 2, len(edges_v) - M // 2):\n",
    "    #     edges_v[i] = edges_v[i] - np.mean(edges_v_copy[i - M // 2:i + M // 2])\n",
    "    # edges_v[:M // 2] = 0\n",
    "    # edges_v[-M // 2:] = 0\n",
    "    # edges_h_copy = edges_h.copy()\n",
    "    # for j in range(M // 2, len(edges_h) - M // 2):\n",
    "    #     edges_h[j] = edges_h[j] - np.mean(edges_h_copy[j - M // 2:j + M // 2])\n",
    "    # edges_h[:M // 2] = 0\n",
    "    # edges_h[-M // 2:] = 0\n",
    "\n",
    "\n",
    "    # cutoff = edge_array.shape[0] * cutoff_percentage // 100\n",
    "    # edges_h = edges_h[cutoff:-cutoff]\n",
    "    # edges_v = edges_v[cutoff:-cutoff]\n",
    "\n",
    "    # peak_v = np.max(np.abs(edges_v))\n",
    "    # peak_h = np.max(np.abs(edges_h))\n",
    "    # edges_v[np.abs(edges_v) < peak_v * peak_threshold] = 0\n",
    "    # edges_h[np.abs(edges_h) < peak_h * peak_threshold] = 0\n",
    "    # sd_v = np.sqrt(np.var(noise_v))\n",
    "    # sd_h = np.sqrt(np.var(noise_h))\n",
    "    # # # sd_h, sd_v = peak_h,peak_v\n",
    "    # x = np.array([edges_v / sd_v, edges_h / sd_h]).reshape(1, -1)\n",
    "    ##############################################################################################################\n",
    "\n",
    "    edges_v_indexes = np.sort(edges_h.argsort()[-total_extracted_features // 2:][::-1])\n",
    "\n",
    "    edges_v_extracted = edges_v[edges_v_indexes]\n",
    "    edges_h_indexes = np.sort(edges_h.argsort()[-total_extracted_features // 2:][::-1])\n",
    "    edges_h_extracted = edges_h[edges_h_indexes]\n",
    "\n",
    "\n",
    "    ## MinMax scaler was used for standardizing the values\n",
    "    # scaler = MinMaxScaler()\n",
    "    # x = np.array([scaler.fit_transform(edges_v_extracted.reshape(-1,1)), scaler.fit_transform(edges_h_extracted.reshape(-1,1))]).reshape(1, -1)\n",
    "\n",
    "    x = np.array([edges_v_extracted, edges_h_extracted]).reshape(-1, 1)\n",
    "    return x\n",
    "\n",
    "def calculate_image_edges2(img, total_extracted_features):\n",
    "    \"\"\"\n",
    "    The edge detection was changed to OpenCV2 functions.\n",
    "    Sobel filter and Canny edge detection were tested.\n",
    "    Canny edge detection achieved results with less noise and more accurate edges.\n",
    "    Then, similar to the previous method, edge values in vertical and horizontal axis were summed up to extract the features.\n",
    "    :param img: source image\n",
    "    :param total_extracted_features: total number of extracted features\n",
    "    :return: the extracted features\n",
    "    \"\"\"\n",
    "\n",
    "    img = np.array(img)\n",
    "    img = np.uint8(img)\n",
    "    # Sharp mask - Enhancing the details in the image by adding the subtraction of blured image from the original image\n",
    "    # img = img + (img - cv2.GaussianBlur(img, (5,5), cv2.BORDER_DEFAULT))\n",
    "    # blurring the source image as the input to the Canny edge detector\n",
    "    blur = cv2.GaussianBlur(img, (5,5), cv2.BORDER_DEFAULT)\n",
    "    edge_array = cv2.Canny(image=blur, threshold1=100, threshold2=200, apertureSize= 3)\n",
    "    edges_v = np.sum(edge_array, axis=0).astype(float)\n",
    "    edges_h = np.sum(edge_array, axis=1).astype(float)\n",
    "    # 32 biggest values from horizontal and vertical values are combined\n",
    "    edges_v_indexes = np.sort(edges_v.argsort()[-total_extracted_features // 2:][::-1])\n",
    "    edges_v_extracted = edges_v[edges_v_indexes]\n",
    "    edges_h_indexes = np.sort(edges_h.argsort()[-total_extracted_features // 2:][::-1])\n",
    "    edges_h_extracted = edges_h[edges_h_indexes]\n",
    "    x = np.array([edges_v_extracted, edges_h_extracted]).reshape(-1, 1)\n",
    "    return x\n",
    "\n",
    "def calculate_image_edges3(img, total_extracted_features):\n",
    "    \"\"\"\n",
    "    This method was developed in combination with the second dynamic cropper function. In this approach,\n",
    "    we don't look for edges, but we sum up the actual pixel values in vertical and horizontal axis.\n",
    "    That is because we used edge detection for finding the actual objects and\n",
    "    using it again seemed to only cause information loss.\n",
    "    :param img: input image as np.array\n",
    "    :param total_extracted_features: total number of extracted features\n",
    "    :return: the extracted features\n",
    "    \"\"\"\n",
    "    vertical = np.sum(img, axis = 0)\n",
    "    horizontal = np.sum(img, axis = 1)\n",
    "    ## tested for noise reduction, did not have positive effects\n",
    "    # vertical[vertical<np.max(vertical) * 0.05] = 0\n",
    "    # horizontal[horizontal<np.max(horizontal) * 0.05] = 0\n",
    "\n",
    "    vertical[vertical == 0] = sorted(set(vertical))[1]\n",
    "    horizontal[horizontal == 0] = sorted(set(horizontal))[1]\n",
    "    edges_v_indexes = np.sort(vertical.argsort()[-total_extracted_features // 2:][::-1])\n",
    "    edges_v_extracted = vertical[edges_v_indexes]\n",
    "    edges_h_indexes = np.sort(horizontal.argsort()[-total_extracted_features // 2:][::-1])\n",
    "    edges_h_extracted = horizontal[edges_h_indexes]\n",
    "    x = np.array([edges_v_extracted, edges_h_extracted])\n",
    "    ## an attempt on normalizing the data - was unreliable\n",
    "    # x = np.log2(x + 1)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def get_data(path_to_training_data,\n",
    "             class_name_chars=3,\n",
    "             test_flag=False,\n",
    "             classes_dict=None,\n",
    "             augment_data=False,\n",
    "             augments=[90, 45]\n",
    "             ):\n",
    "    \"\"\"\n",
    "    Reads the images. Adds rotation to them if augment_data flag is True.\n",
    "    This way, we can make up for the low number of training images and take the rotation in images into account.\n",
    "    :param path_to_training_data:\n",
    "    :param class_name_chars:\n",
    "    :param test_flag:\n",
    "    :param classes_dict:\n",
    "    :param augment_data: Flag to augmenting data\n",
    "    :param augments: The angles we want to rotate the image by\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    image_names, classes, classes_dict = get_files_list(path_to_training_data, class_name_chars, test_flag=test_flag,\n",
    "                                                        classes_dict=classes_dict)\n",
    "    new_classes = []\n",
    "    for i, name in enumerate(image_names):\n",
    "        img = image_reader(str(Path(path_to_training_data) / name))\n",
    "        data.append(np.array(img))\n",
    "        new_classes.append(classes[i])\n",
    "        if augment_data:\n",
    "            for value in augments:\n",
    "                rotated = ndimage.rotate(img, value)\n",
    "                rotated = np.array(rotated)\n",
    "                rotated[rotated == 0] = np.mean(np.array(img)[0:20, 0:20])\n",
    "                data.append(rotated)\n",
    "                new_classes.append(classes[i])\n",
    "\n",
    "    # converting classes list to a column array\n",
    "    new_classes = np.array(new_classes).reshape(-1, 1)\n",
    "    return np.array(data, dtype=object), new_classes, classes_dict\n",
    "\n",
    "\n",
    "def preprocess_data(data,\n",
    "                    standardization_flag=True,\n",
    "                    standardization_max_value=255,\n",
    "                    crop_flag=True,\n",
    "                    crop_height_percentage=50,\n",
    "                    crop_width_percentage=50,\n",
    "                    scaling_flag=True,\n",
    "                    scaling_height=100,\n",
    "                    scaling_width=100,\n",
    "                    total_extracted_features=64,\n",
    "                    region_of_interest=500,\n",
    "                    scaling = True,\n",
    "                    scaler=None):\n",
    "    \"\"\"\n",
    "    The actual preprocessing pipeline for the model. The images are standardized to 0-255.\n",
    "    Then scaled, cropped by the chosen method, and finally the 64 features are extracted.\n",
    "    For the first dynamic image cropper, first the image is cropped and then scaled,\n",
    "    while for the dynamic image cropper 2, first the image is scaled and then cropped\n",
    "    :param data:\n",
    "    :param standardization_flag:\n",
    "    :param standardization_max_value:\n",
    "    :param crop_flag:\n",
    "    :param crop_height_percentage:\n",
    "    :param crop_width_percentage:\n",
    "    :param scaling_flag:\n",
    "    :param scaling_height:\n",
    "    :param scaling_width:\n",
    "    :param total_extracted_features:\n",
    "    :param region_of_interest: used for dynamic image cropper\n",
    "    :param scaling: If true, the extracted feature are scaled using a MonMaxScaler\n",
    "    :param scaler: we pass in the scaler if we have a pre-fitted scaler\n",
    "    :return: training and testing data\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    for img in data:\n",
    "        if standardization_flag:\n",
    "            img = image_standardization(img, standardization_max_value)\n",
    "        if scaling_flag:\n",
    "            img = image_scaler(img, scaling_height, scaling_width)\n",
    "        if crop_flag:\n",
    "            # img = np.array(img)\n",
    "            img = dynamic_image_cropper(img, region_of_interest=region_of_interest,negative_image=True)\n",
    "            # img = dynamic_image_cropper2(img)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # img = Image.fromarray(img).convert('L')\n",
    "        # img = calculate_image_edges(img, total_extracted_features)\n",
    "        # img = calculate_image_edges2(img, total_extracted_features)\n",
    "        img = calculate_image_edges3(img, total_extracted_features)\n",
    "        img = img.flatten()\n",
    "        new_data.append(img)\n",
    "    new_data = np.array(new_data)\n",
    "    if scaling:\n",
    "        if scaler is None:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(new_data)\n",
    "        new_data = scaler.transform(new_data)\n",
    "    return new_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train.shape = (81, 64), y_train.shape = 81\n",
      "\n",
      "Confusion Matrix = \n",
      "[[18  2  7]\n",
      " [ 0 25  2]\n",
      " [ 2  6 19]]\n",
      "\n",
      "Accuracy Score = 0.7654320987654321\n",
      "Model Coefficient Shape = (3, 64)\n"
     ]
    }
   ],
   "source": [
    "# The train function uses the get_data function as a base to read all the images from the given directory, create the classes, prepare the dataset, and then train a logistic regression model to classify the images. In the end, it returns the trained model and classes_dict to be passed to the test function\n",
    "\n",
    "# control parameters for the preprocessing functions\n",
    "region_of_interest = 600\n",
    "scaling_height = 150\n",
    "scaling_width = 150\n",
    "####################################################\n",
    "\n",
    "\n",
    "def train_function(path_to_data,\n",
    "                   class_name_chars=3,\n",
    "                   test_flag=False,\n",
    "                   classes_dict=None,\n",
    "                   standardization_flag=True,\n",
    "                   standardization_max_value=255,\n",
    "                   crop_flag=True,\n",
    "                   crop_height_percentage=15,\n",
    "                   crop_width_percentage=15,\n",
    "                   scaling_flag=True,\n",
    "                   scaling_height=scaling_height,\n",
    "                   scaling_width=scaling_width,\n",
    "                   region_of_interest=region_of_interest,\n",
    "                   total_extracted_features=64,\n",
    "                   results=None,\n",
    "                   scaling = True,\n",
    "                   scaler=None):\n",
    "    data, y_train, classes_dict = get_data(path_to_data, class_name_chars, test_flag, classes_dict)\n",
    "    X_train, scaler = preprocess_data(data, standardization_flag=standardization_flag,\n",
    "                                      standardization_max_value=standardization_max_value, crop_flag=crop_flag,\n",
    "                                      crop_height_percentage=crop_height_percentage,\n",
    "                                      crop_width_percentage=crop_width_percentage, scaling_flag=scaling_flag,\n",
    "                                      scaling_height=scaling_height, scaling_width=scaling_width,\n",
    "                                      total_extracted_features=total_extracted_features,\n",
    "                                      region_of_interest=region_of_interest, scaling = scaling, scaler=scaler)\n",
    "    # train LogisticRegression model\n",
    "    model = LogisticRegression(max_iter=5000000)\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "\n",
    "    print(f'\\nX_train.shape = {X_train.shape}, y_train.shape = {len(y_train)}')\n",
    "    y_pred = model.predict(X_train)\n",
    "\n",
    "    # Reporting the stats from the model\n",
    "    print(f'\\nConfusion Matrix = \\n{confusion_matrix(y_train, y_pred)}')\n",
    "    print(f'\\nAccuracy Score = {accuracy_score(y_train, y_pred)}')\n",
    "    print(f'Model Coefficient Shape = {model.coef_.shape}')\n",
    "    if results is not None:\n",
    "        results.append(accuracy_score(y_train, y_pred))\n",
    "    return model, classes_dict, results, scaler\n",
    "\n",
    "\n",
    "model, classes_dict, results, scaler = train_function('data/Lego_dataset_2/training/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_test.shape = (81, 64), y_test.shape = 81\n",
      "\n",
      "Confusion Matrix = \n",
      "[[21  0  6]\n",
      " [ 5 18  4]\n",
      " [ 2  2 23]]\n",
      "\n",
      "Accuracy Score = 0.7654320987654321\n",
      "Model Coefficient Shape = (3, 64)\n"
     ]
    }
   ],
   "source": [
    "# The test function uses the get_data function as a base to read all the images from the given directory, get their classes based on the classes_dict from the training function, prepare the dataset, and then predict the classes of the images.\n",
    "\n",
    "def test_function(path_to_data,\n",
    "                  model,\n",
    "                  class_name_chars=3,\n",
    "                  test_flag=False,\n",
    "                  classes_dict=None,\n",
    "                   standardization_flag=True,\n",
    "                  standardization_max_value=255,\n",
    "                  crop_flag=True,\n",
    "                  crop_height_percentage=10,\n",
    "                  crop_width_percentage=10,\n",
    "                  scaling_flag=True,\n",
    "                  scaling_height=scaling_height,\n",
    "                  scaling_width=scaling_width,\n",
    "                  region_of_interest=region_of_interest,\n",
    "                  total_extracted_features=64,\n",
    "                  results=None,\n",
    "                  scaling = True,\n",
    "                  scaler = None):\n",
    "    data, y_test, classes_dict = get_data(path_to_data, class_name_chars, test_flag, classes_dict, augment_data= False)\n",
    "    X_test, scaler = preprocess_data(data, standardization_flag=standardization_flag,\n",
    "                             standardization_max_value=standardization_max_value, crop_flag=crop_flag,\n",
    "                             crop_height_percentage=crop_height_percentage,\n",
    "                             crop_width_percentage=crop_width_percentage, scaling_flag=scaling_flag,\n",
    "                             scaling_height=scaling_height, scaling_width=scaling_width,\n",
    "                             total_extracted_features=total_extracted_features, region_of_interest=region_of_interest, scaling = scaling, scaler = scaler)\n",
    "    # train LogisticRegression model\n",
    "    print(f'\\nX_test.shape = {X_test.shape}, y_test.shape = {len(y_test)}')\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Reporting the stats from the model\n",
    "    print(f'\\nConfusion Matrix = \\n{confusion_matrix(y_test, y_pred)}')\n",
    "    print(f'\\nAccuracy Score = {accuracy_score(y_test, y_pred)}')\n",
    "    print(f'Model Coefficient Shape = {model.coef_.shape}')\n",
    "    if results is not None:\n",
    "        results.append(accuracy_score(y_test, y_pred))\n",
    "    return results\n",
    "\n",
    "\n",
    "results = test_function('data/Lego_dataset_2/testing/', model, test_flag=True, classes_dict=classes_dict, scaler=scaler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
