{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Applied Machine Learning \n",
    "## Project â€“ Stage 1 (Sorting Lego Pieces Using Raw Images)\n",
    "\n",
    "We have been hired by a company to develop machine learning algorithms for a sorting facility. The requirement is that the sorting device takes images of items on a conveyor belt, and then uses and machine learning algorithm to classify the items into classes. Then, the items get routed through different routes on the conveyor belt depending on their class.\n",
    "\n",
    "For the sake of this project, we are given RGB images, and our focus will be on developing the classification algorithms. Also, for simplicity, it is assumed that items are Lego pieces of three different types with the following shapes (top view): Rectangles (2x4), squares (2x2), and circles (2x2). Examples are shown below.\n",
    "\n",
    "<img align=\"center\" src=\"Lego.JPG\">\n",
    "\n",
    "This algorithm classifies these three classes with an acceptable level of accuracy. It uses the raw image (grayscale conversion and scaling/cropping are acceptable) as an input to a single neuron classifier with < ðŸ’ðŸŽðŸ—ðŸ• weights (trainable parameters). We are given two datasets to achieve this goal, each containing multiple images of each class. The dataset in the folder â€˜trainingâ€™ for training, and the one in the folder â€˜testingâ€™ for testing, are used. The names of the folders or files should not be changed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The required libraries are imported here\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import PIL\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell contains the functions defined to read the images from the given directories,\n",
    "# separate them into classes, and preprocess them so they can be used by the model.\n",
    "\n",
    "def image_cropper(img: np.array, height_percentage: int = 40, width_percentage: int = 40) -> np.array:\n",
    "    \"\"\"\n",
    "    the image is cropped from both sides of the height and width\n",
    "    :param img: the images as an np.array\n",
    "    :param width_percentage: the percentage of the pixels from the width of the image we would like to crop\n",
    "    :param height_percentage: the percentage of the pixels from the height of the image we would like to crop\n",
    "    :return: cropped image as np.array\n",
    "    \"\"\"\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    ### print(img.shape)\n",
    "    cropped_height_amount = height // 100 * height_percentage\n",
    "    cropped_width_amount = height // 100 * width_percentage\n",
    "    img = img[cropped_height_amount // 2:height - cropped_height_amount // 2,\n",
    "          cropped_width_amount // 2:width - cropped_width_amount // 2]\n",
    "    return img\n",
    "\n",
    "\n",
    "def image_scaler(img, new_height=20, new_width=20):\n",
    "    \"\"\"\n",
    "    changes the image scale\n",
    "    :param img: the images as an np.array\n",
    "    :param new_height: the new height of the scaled image\n",
    "    :param new_width: the new width of the scaled image\n",
    "    :return: scaled image as np.array\n",
    "    \"\"\"\n",
    "    img = PIL.Image.fromarray(img)\n",
    "    img = img.resize((new_width, new_height))\n",
    "    img = np.array(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def image_standardization(img, max_value=255):\n",
    "    \"\"\"\n",
    "    changes the values of the image pixels to be between 0 and max_value\n",
    "    :param max_value: max_value of the new pixels\n",
    "    :param img: the images as an np.array\n",
    "    :return: standardized image\n",
    "    \"\"\"\n",
    "    ### print(type(img))\n",
    "    img = (img - np.min(img)) / (np.max(img) - np.min(img)) * max_value\n",
    "    ### print(type(img))\n",
    "    #A# applying numpy operations changes the image format from 'PIL.Image.Image' to 'numpy.ndarray'\n",
    "    ### print(img.shape)\n",
    "    return img\n",
    "\n",
    "\n",
    "def separate_classes(img_files, name_separation_chars=3, test_flag=False, classes_dict=None):\n",
    "    \"\"\"\n",
    "    separates the images into classes based on their file names\n",
    "    :param img_files: list of image file names\n",
    "    :param name_separation_chars: number of characters in the name that should get used for separation\n",
    "    :return: a list of classes for image names, a dict containing the name and the corresponding class number of images\n",
    "    \"\"\"\n",
    "    names = list(set(img_name[:name_separation_chars] for img_name in img_files))\n",
    "    ### print(names); print(len(names))\n",
    "    if not test_flag:\n",
    "        classes_dict = dict(enumerate(names))\n",
    "        ### print(enumerate(names)); print(classes_dict)\n",
    "        # changing the order of keys and items in the dict\n",
    "        classes_dict = {v: k for k, v in classes_dict.items()}\n",
    "        ### print(classes_dict)\n",
    "    classes = []\n",
    "    for img_name in img_files:\n",
    "        classes.append(classes_dict[img_name[:name_separation_chars]])    \n",
    "    ### print(classes); print(len(classes))\n",
    "    return classes, classes_dict\n",
    "\n",
    "\n",
    "def get_files_list(data_dir, name_separation_chars=3, test_flag=False, classes_dict=None):\n",
    "    \"\"\"\n",
    "    gets the list of image names from the directory\n",
    "    :param data_dir: path to the images file\n",
    "    :return: list of image names, their classes and the dict for classes\n",
    "    \"\"\"\n",
    "    data = os.listdir(data_dir)\n",
    "    ### print(data, len(data))\n",
    "    classes, classes_dict = separate_classes(data, name_separation_chars=name_separation_chars, test_flag=test_flag,\n",
    "                                             classes_dict=classes_dict)\n",
    "    return data, classes, classes_dict\n",
    "\n",
    "\n",
    "def image_reader(img_dir):\n",
    "    \"\"\"\n",
    "    reads image in grayscale\n",
    "    :param img_dir: path to the images\n",
    "    :return: img as np.array\n",
    "    \"\"\"\n",
    "    img = PIL.Image.open(img_dir)\n",
    "    gray_img = img.convert(\"L\")\n",
    "    ### print(np.array(gray_img)); print(type(np.array(gray_img))); print(np.array(gray_img).shape)\n",
    "    ### print(gray_img); print(type(gray_img))\n",
    "    return gray_img\n",
    "\n",
    "\n",
    "def get_data(path_to_training_data,\n",
    "             class_name_chars=3,\n",
    "             test_flag=False,\n",
    "             classes_dict=None,\n",
    "             standardization_flag=True,\n",
    "             standardization_max_value=255,\n",
    "             crop_flag=True,\n",
    "             crop_height_percentage=50,\n",
    "             crop_width_percentage=50,\n",
    "             scaling_flag=True,\n",
    "             scaling_height=64,\n",
    "             scaling_width=64):\n",
    "    data = []\n",
    "    image_names, classes, classes_dict = get_files_list(path_to_training_data, class_name_chars, test_flag=test_flag,\n",
    "                                                        classes_dict=classes_dict)\n",
    "    for name in image_names:\n",
    "        img = image_reader(str(Path(path_to_training_data) / name))\n",
    "        ### print(str(Path(path_to_training_data) / name))\n",
    "        ### img.show()\n",
    "        if standardization_flag:\n",
    "            img = image_standardization(img, standardization_max_value)\n",
    "        if crop_flag:\n",
    "            img = image_cropper(img, crop_height_percentage, crop_width_percentage)\n",
    "        if scaling_flag:\n",
    "            img = image_scaler(img, scaling_height, scaling_width)\n",
    "        ### print(img)\n",
    "        data.append(img.flatten())\n",
    "    # converting classes list to a column array\n",
    "    classes = np.array(classes).reshape(-1, 1)\n",
    "    return np.array(data), classes, classes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train.shape = (108, 2500), y_train.shape = (108, 1)\n",
      "\n",
      "Confusion Matrix = \n",
      "[[36  0  0]\n",
      " [ 0 36  0]\n",
      " [ 0  0 36]]\n",
      "\n",
      "Accuracy Score = 1.0\n",
      "Model Coefficient Shape = (3, 2500)\n"
     ]
    }
   ],
   "source": [
    "# The train function uses the get_data function as a base to read all the images from the given directory, create the classes, prepare the dataset, and then train a logistic regression model to classify the images. In the end, it returns the trained model and classes_dict to be passed to the test function\n",
    "\n",
    "def train_function(path_to_data,\n",
    "                   class_name_chars=3,\n",
    "                   test_flag=False,\n",
    "                   classes_dict=None,\n",
    "                   standardization_flag=True,\n",
    "                   standardization_max_value=255,\n",
    "                   crop_flag=True,\n",
    "                   crop_height_percentage=50,\n",
    "                   crop_width_percentage=50,\n",
    "                   scaling_flag=True,\n",
    "                   scaling_height=50,\n",
    "                   scaling_width=50, results = None):\n",
    "    X_train, y_train, classes_dict = get_data(path_to_data, class_name_chars, test_flag, classes_dict,\n",
    "                                              standardization_flag, standardization_max_value, crop_flag,\n",
    "                                              crop_height_percentage, crop_width_percentage, scaling_flag,\n",
    "                                              scaling_height, scaling_width)\n",
    "    # train LogisticRegression model\n",
    "    model = LogisticRegression(max_iter=10000)\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "\n",
    "    print(f'\\nX_train.shape = {X_train.shape}, y_train.shape = {y_train.shape}')\n",
    "    y_pred = model.predict(X_train)\n",
    "\n",
    "    # Reporting the stats from the model\n",
    "    print(f'\\nConfusion Matrix = \\n{confusion_matrix(y_train, y_pred)}')\n",
    "    print(f'\\nAccuracy Score = {accuracy_score(y_train, y_pred)}')\n",
    "    print(f'Model Coefficient Shape = {model.coef_.shape}')\n",
    "    if results is not None:\n",
    "        results.append(accuracy_score(y_train, y_pred))\n",
    "    return model, classes_dict, results\n",
    "\n",
    "\n",
    "model, classes_dict, results = train_function('data/Lego_dataset_1/training/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_test.shape = (54, 2500), y_test.shape = (54, 1)\n",
      "\n",
      "Confusion Matrix = \n",
      "[[17  0  1]\n",
      " [ 0 17  1]\n",
      " [ 0  1 17]]\n",
      "\n",
      "Accuracy Score = 0.9444444444444444\n",
      "Model Coefficient Shape = (3, 2500)\n"
     ]
    }
   ],
   "source": [
    "# The test function uses the get_data function as a base to read all the images from the given directory, get their classes based on the classes_dict from the training function, prepare the dataset, and then predict the classes of the images.\n",
    "\n",
    "def test_function(path_to_data,\n",
    "                  model,\n",
    "                  class_name_chars=3,\n",
    "                  test_flag=False,\n",
    "                  classes_dict=None,\n",
    "                  standardization_flag=True,\n",
    "                  standardization_max_value=255,\n",
    "                  crop_flag=True,\n",
    "                  crop_height_percentage=50,\n",
    "                  crop_width_percentage=50,\n",
    "                  scaling_flag=True,\n",
    "                  scaling_height=50,\n",
    "                  scaling_width=50, results=None):\n",
    "    X_test, y_test, classes_dict = get_data(path_to_data, class_name_chars, test_flag, classes_dict,\n",
    "                                            standardization_flag, standardization_max_value, crop_flag,\n",
    "                                            crop_height_percentage, crop_width_percentage, scaling_flag, scaling_height,\n",
    "                                            scaling_width)\n",
    "    # train LogisticRegression model\n",
    "    print(f'\\nX_test.shape = {X_test.shape}, y_test.shape = {y_test.shape}')\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Reporting the stats from the model\n",
    "    print(f'\\nConfusion Matrix = \\n{confusion_matrix(y_test, y_pred)}')\n",
    "    print(f'\\nAccuracy Score = {accuracy_score(y_test, y_pred)}')\n",
    "    print(f'Model Coefficient Shape = {model.coef_.shape}')\n",
    "    if results is not None:\n",
    "        results.append(accuracy_score(y_test, y_pred))\n",
    "    return results\n",
    "\n",
    "\n",
    "results = test_function('data/Lego_dataset_1/testing/', model, test_flag=True, classes_dict=classes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Training the model with different number of inputs\n",
    "train_results = []\n",
    "test_results = []\n",
    "parameters = []\n",
    "for scale in range(5, 64, 5):\n",
    "    for crop in range(0, 80, 5):\n",
    "        print(f'crop: {crop},scale: {scale}')\n",
    "        parameters.append(([crop,scale]))\n",
    "        model, classes_dict, results = train_function('data/Lego_dataset_1/training/', crop_height_percentage=crop,\n",
    "                                             crop_width_percentage=crop, scaling_width=scale, scaling_height=scale, results = train_results)\n",
    "        test_results = test_function('data/Lego_dataset_1/testing/', model, test_flag=True, classes_dict=classes_dict,\n",
    "                      crop_height_percentage=crop, crop_width_percentage=crop, scaling_width=scale,\n",
    "                      scaling_height=scale, results=test_results)\n",
    "        print('----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell was just used to get the training and testing data, for creating the excel file\n",
    "\n",
    "train_results_arr = np.array(train_results).reshape(-1,1)\n",
    "test_results_arr = np.array(test_results).reshape(-1,1)\n",
    "parameters_arr = np.array(parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
